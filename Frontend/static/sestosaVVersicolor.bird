fn exp(x: float) -> float {
  if x < -30.0 {
      return 0.0;
  } else if x > 30.0 {
      return exp(30.0);
  }

  var result = 1.0;
  var term = 1.0;
  for var i = 1; i < 30; i += 1 {
      term = term * x / i as float;
      result += term;
  }
  return result;
}

fn sqrt(x: float) -> float {
  var z: float = 1.0;
  for var i = 1; i <= 30; i += 1 {
      z -= (z*z - x) / (2.0 *z); // MAGIC LINE!!
  }
  return z;
}

fn log(x: float) -> float {
  var y = (x - 1.0) / (x + 1.0);
  var y_squared = y * y;
  var result = 0.0;

  for var i = 1; i < 30; i += 2 {
      var x = 1.0 / i as float;
      for var j = 1; j < i; j += 1 {
          x *= y_squared;
      }
      result += x;
  }

  return 2.0 * y * result;
}

fn sigmoid(x: float) -> float {
  return 1.0 / (1.0 + exp(-x));
}

fn abs(x: float) ->  float {
return (x < 0.0) ? (-x) : x;
}

fn predict(x: float[], weights: float[], n: int) -> float {
  var z = 0.0;
  for var i = 0; i < n; i += 1 {
      z += weights[i] * x[i];
  }
  return sigmoid(z);
}

fn evaluate(X: float[], y: int[], weights: float[], m: int, n: int) -> float {
  var correct = 0;
  var acc = 0.0;
  for var i = 0; i < m; i += 1 {
      var x: float[] = [];
      for var j = 0; j < n; j += 1 {
          push(x, (X[i * n + j]));
      }

      var p = predict(x, weights, n);
      var predicted = 0;
      if p >= 0.5 {
          predicted = 1;
      }

      if predicted == y[i] {
          correct += 1;
      }
  }

  acc = correct as float / m as float;
  return acc;
}

fn gradient_decent(X: float[], y: int[], weights: float[], m: int, 
                 n: int, learning_rate: float, iterations: int) -> void {
  var prev = 0.0;
  for var i = 0; i < iterations; i += 1 {
      for var j = 0; j < n; j += 1 {
          var gradient = 0.0;

          for var i = 0; i < m; i += 1 {
              var z: float = 0.0;
              for var k = 0; k < n; k += 1 {
                  z += weights[k] * X[i * n + k];
              }

              var h = sigmoid(z);
              gradient += (h - y[i] as float) * X[i * n + j];
          }

          weights[j] -= learning_rate * gradient / m as float;
      }
      if i % 50 == 0 {
          var acc = evaluate(X, y, weights, m, n);
          var percent = 100.0 * acc;
          if (acc != prev) {
            print "accuracy: ", percent, "%";
          }
          prev = acc;
          if acc == 1.0 {
              print "converged!";
              return;
          }
      }
  }
}

fn normalize(X: float[], m: int, n: int) -> float[] {
  var means: float[] = [];
  var stds: float[] = [];

  push(means, 0.0);
  push(stds, 1.0);

  for var j = 1; j < n; j += 1 {
      var sum = 0.0;
      for var i = 0; i < m; i += 1 {
          sum += X[i * n + j];
      }
      var mean = sum / m as float;
      push(means, mean);

      var sq_sum = 0.0;
      for var i = 0; i < m; i += 1 {
          var diff = X[i * n + j] - mean;
          sq_sum += diff * diff;
      }
      var std = sqrt(sq_sum / m as float);
      if std == 0.0 {
          std = 1.0;
      }
      push(stds, std);
  }

  var X_norm: float[] = [];
  for var i = 0; i < m; i += 1 {
      for var j = 0; j < n; j += 1 {
          var val = X[i * n + j];
          var normalized = (j == 0) ? 1.0 : (val - means[j]) / stds[j];
          push(X_norm, normalized);
      }
  }

  return X_norm;
}

var X: float[] = [
    1.000000, -1.012978, -1.042111,
    1.000000, -1.012978, -1.042111,
    1.000000, -1.082312, -1.042111,
    1.000000, -0.943643, -1.042111,
    1.000000, -1.012978, -1.042111,
    1.000000, -0.804974, -0.686442,
    1.000000, -1.012978, -0.864276,
    1.000000, -0.943643, -1.042111,
    1.000000, -1.012978, -1.042111,
    1.000000, -0.943643, -1.219946,
    1.000000, -0.943643, -1.042111,
    1.000000, -0.874309, -1.042111,
    1.000000, -1.012978, -1.219946,
    1.000000, -1.220981, -1.219946,
    1.000000, -1.151647, -1.042111,
    1.000000, -0.943643, -0.686442,
    1.000000, -1.082312, -0.686442,
    1.000000, -1.012978, -0.864276,
    1.000000, -0.804974, -0.864276,
    1.000000, -0.943643, -0.864276,
    1.000000, -0.804974, -1.042111,
    1.000000, -0.943643, -0.686442,
    1.000000, -1.290316, -1.042111,
    1.000000, -0.804974, -0.508607,
    1.000000, -0.666305, -1.042111,
    1.000000, -0.874309, -1.042111,
    1.000000, -0.874309, -0.686442,
    1.000000, -0.943643, -1.042111,
    1.000000, -1.012978, -1.042111,
    1.000000, -0.874309, -1.042111,
    1.000000, -0.874309, -1.042111,
    1.000000, -0.943643, -0.686442,
    1.000000, -0.943643, -1.219946,
    1.000000, -1.012978, -1.042111,
    1.000000, -0.943643, -1.042111,
    1.000000, -1.151647, -1.042111,
    1.000000, -1.082312, -1.042111,
    1.000000, -1.012978, -1.219946,
    1.000000, -1.082312, -1.042111,
    1.000000, -0.943643, -1.042111,
    1.000000, -1.082312, -0.864276,
    1.000000, -1.082312, -0.864276,
    1.000000, -1.082312, -1.042111,
    1.000000, -0.874309, -0.330772,
    1.000000, -0.666305, -0.686442,
    1.000000, -1.012978, -0.864276,
    1.000000, -0.874309, -1.042111,
    1.000000, -1.012978, -1.042111,
    1.000000, -0.943643, -1.042111,
    1.000000, -1.012978, -1.042111,
    1.000000, 1.275062, 1.091905,
    1.000000, 1.136393, 1.269739,
    1.000000, 1.413731, 1.269739,
    1.000000, 0.789720, 0.914070,
    1.000000, 1.205728, 1.269739,
    1.000000, 1.136393, 0.914070,
    1.000000, 1.275062, 1.447574,
    1.000000, 0.304379, 0.380566,
    1.000000, 1.205728, 0.914070,
    1.000000, 0.720386, 1.091905,
    1.000000, 0.443048, 0.380566,
    1.000000, 0.928390, 1.269739,
    1.000000, 0.789720, 0.380566,
    1.000000, 1.275062, 1.091905,
    1.000000, 0.512382, 0.914070,
    1.000000, 1.067059, 1.091905,
    1.000000, 1.136393, 1.269739,
    1.000000, 0.859055, 0.380566,
    1.000000, 1.136393, 1.269739,
    1.000000, 0.720386, 0.558401,
    1.000000, 1.344397, 1.803243,
    1.000000, 0.789720, 0.914070,
    1.000000, 1.413731, 1.269739,
    1.000000, 1.275062, 0.736235,
    1.000000, 0.997724, 0.914070,
    1.000000, 1.067059, 1.091905,
    1.000000, 1.344397, 1.091905,
    1.000000, 1.483066, 1.625408,
    1.000000, 1.136393, 1.269739,
    1.000000, 0.443048, 0.380566,
    1.000000, 0.651051, 0.558401,
    1.000000, 0.581717, 0.380566,
    1.000000, 0.720386, 0.736235,
    1.000000, 1.552400, 1.447574,
    1.000000, 1.136393, 1.269739,
    1.000000, 1.136393, 1.447574,
    1.000000, 1.275062, 1.269739,
    1.000000, 1.067059, 0.914070,
    1.000000, 0.859055, 0.914070,
    1.000000, 0.789720, 0.914070,
    1.000000, 1.067059, 0.736235,
    1.000000, 1.205728, 1.091905,
    1.000000, 0.789720, 0.736235,
    1.000000, 0.304379, 0.380566,
    1.000000, 0.928390, 0.914070,
    1.000000, 0.928390, 0.736235,
    1.000000, 0.928390, 0.914070,
    1.000000, 0.997724, 0.914070,
    1.000000, 0.096375, 0.558401,
    1.000000, 0.859055, 0.914070
];

var y: int[] = [
    0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1
];

var weights = [0.0, 0.0, 0.0]; 
var learning_rate = 0.05;
var iterations = 100;
var m = length(X) / length(weights);
var n = length(weights);

X = normalize(X, m, n);

gradient_decent(X, y, weights, m, n, learning_rate, iterations);

for var i = 0; i < m; i += 1 {
  var x: float[] = [];
  for var j = 0; j < n; j += 1 {
      push(x, X[i * n + j]);
  }
  var p: float = predict(x, weights, n);
}

var acc = evaluate(X, y, weights, m, n);
var percent = 100.0 * acc;

print "w0 = ", weights[0];
print "w1 = ", weights[1];
print "w2 = ", weights[2];

print "final accuracy: ", percent, "%";