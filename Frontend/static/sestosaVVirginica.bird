fn exp(x: float) -> float {
  if x < -30.0 {
      return 0.0;
  } else if x > 30.0 {
      return exp(30.0);
  }

  var result = 1.0;
  var term = 1.0;
  for var i = 1; i < 30; i += 1 {
      term = term * x / i as float;
      result += term;
  }
  return result;
}

fn sqrt(x: float) -> float {
  var z: float = 1.0;
  for var i = 1; i <= 30; i += 1 {
      z -= (z*z - x) / (2.0 *z); // MAGIC LINE!!
  }
  return z;
}

fn log(x: float) -> float {
  var y = (x - 1.0) / (x + 1.0);
  var y_squared = y * y;
  var result = 0.0;

  for var i = 1; i < 30; i += 2 {
      var x = 1.0 / i as float;
      for var j = 1; j < i; j += 1 {
          x *= y_squared;
      }
      result += x;
  }

  return 2.0 * y * result;
}

fn sigmoid(x: float) -> float {
  return 1.0 / (1.0 + exp(-x));
}

fn abs(x: float) ->  float {
return (x < 0.0) ? (-x) : x;
}

fn predict(x: float[], weights: float[], n: int) -> float {
  var z = 0.0;
  for var i = 0; i < n; i += 1 {
      z += weights[i] * x[i];
  }
  return sigmoid(z);
}

fn evaluate(X: float[], y: int[], weights: float[], m: int, n: int) -> float {
  var correct = 0;
  var acc = 0.0;
  for var i = 0; i < m; i += 1 {
      var x: float[] = [];
      for var j = 0; j < n; j += 1 {
          push(x, (X[i * n + j]));
      }

      var p = predict(x, weights, n);
      var predicted = 0;
      if p >= 0.5 {
          predicted = 1;
      }

      if predicted == y[i] {
          correct += 1;
      }
  }

  acc = correct as float / m as float;
  return acc;
}

fn gradient_decent(X: float[], y: int[], weights: float[], m: int, 
                 n: int, learning_rate: float, iterations: int) -> void {
  var prev = 0.0;
  for var i = 0; i < iterations; i += 1 {
      for var j = 0; j < n; j += 1 {
          var gradient = 0.0;

          for var i = 0; i < m; i += 1 {
              var z: float = 0.0;
              for var k = 0; k < n; k += 1 {
                  z += weights[k] * X[i * n + k];
              }

              var h = sigmoid(z);
              gradient += (h - y[i] as float) * X[i * n + j];
          }

          weights[j] -= learning_rate * gradient / m as float;
      }
      if i % 50 == 0 {
          var acc = evaluate(X, y, weights, m, n);
          var percent = 100.0 * acc;
          if (acc != prev) {
            print "accuracy: ", percent, "%";
          }
          prev = acc;
          if acc == 1.0 {
              print "converged!";
              return;
          }
      }
  }
}

fn normalize(X: float[], m: int, n: int) -> float[] {
  var means: float[] = [];
  var stds: float[] = [];

  push(means, 0.0);
  push(stds, 1.0);

  for var j = 1; j < n; j += 1 {
      var sum = 0.0;
      for var i = 0; i < m; i += 1 {
          sum += X[i * n + j];
      }
      var mean = sum / m as float;
      push(means, mean);

      var sq_sum = 0.0;
      for var i = 0; i < m; i += 1 {
          var diff = X[i * n + j] - mean;
          sq_sum += diff * diff;
      }
      var std = sqrt(sq_sum / m as float);
      if std == 0.0 {
          std = 1.0;
      }
      push(stds, std);
  }

  var X_norm: float[] = [];
  for var i = 0; i < m; i += 1 {
      for var j = 0; j < n; j += 1 {
          var val = X[i * n + j];
          var normalized = (j == 0) ? 1.0 : (val - means[j]) / stds[j];
          push(X_norm, normalized);
      }
  }

  return X_norm;
}

var X: float[] = [
    1.000000, -1.010688, -1.024617,
    1.000000, -1.010688, -1.024617,
    1.000000, -1.058656, -1.024617,
    1.000000, -0.962720, -1.024617,
    1.000000, -1.010688, -1.024617,
    1.000000, -0.866784, -0.805682,
    1.000000, -1.010688, -0.915150,
    1.000000, -0.962720, -1.024617,
    1.000000, -1.010688, -1.024617,
    1.000000, -0.962720, -1.134085,
    1.000000, -0.962720, -1.024617,
    1.000000, -0.914752, -1.024617,
    1.000000, -1.010688, -1.134085,
    1.000000, -1.154592, -1.134085,
    1.000000, -1.106624, -1.024617,
    1.000000, -0.962720, -0.805682,
    1.000000, -1.058656, -0.805682,
    1.000000, -1.010688, -0.915150,
    1.000000, -0.866784, -0.915150,
    1.000000, -0.962720, -0.915150,
    1.000000, -0.866784, -1.024617,
    1.000000, -0.962720, -0.805682,
    1.000000, -1.202560, -1.024617,
    1.000000, -0.866784, -0.696214,
    1.000000, -0.770847, -1.024617,
    1.000000, -0.914752, -1.024617,
    1.000000, -0.914752, -0.805682,
    1.000000, -0.962720, -1.024617,
    1.000000, -1.010688, -1.024617,
    1.000000, -0.914752, -1.024617,
    1.000000, -0.914752, -1.024617,
    1.000000, -0.962720, -0.805682,
    1.000000, -0.962720, -1.134085,
    1.000000, -1.010688, -1.024617,
    1.000000, -0.962720, -1.024617,
    1.000000, -1.106624, -1.024617,
    1.000000, -1.058656, -1.024617,
    1.000000, -1.010688, -1.134085,
    1.000000, -1.058656, -1.024617,
    1.000000, -0.962720, -1.024617,
    1.000000, -1.058656, -0.915150,
    1.000000, -1.058656, -0.915150,
    1.000000, -1.058656, -1.024617,
    1.000000, -0.914752, -0.586747,
    1.000000, -0.770847, -0.805682,
    1.000000, -1.010688, -0.915150,
    1.000000, -0.914752, -1.024617,
    1.000000, -1.010688, -1.024617,
    1.000000, -0.962720, -1.024617,
    1.000000, -1.010688, -1.024617,
    1.000000, 1.195845, 1.493139,
    1.000000, 0.764132, 0.836333,
    1.000000, 1.147877, 1.055268,
    1.000000, 1.003973, 0.726865,
    1.000000, 1.099909, 1.164736,
    1.000000, 1.483654, 1.055268,
    1.000000, 0.476323, 0.617398,
    1.000000, 1.339749, 0.726865,
    1.000000, 1.099909, 0.726865,
    1.000000, 1.243813, 1.493139,
    1.000000, 0.764132, 0.945800,
    1.000000, 0.860068, 0.836333,
    1.000000, 0.956004, 1.055268,
    1.000000, 0.716164, 0.945800,
    1.000000, 0.764132, 1.383671,
    1.000000, 0.860068, 1.274203,
    1.000000, 0.956004, 0.726865,
    1.000000, 1.531622, 1.164736,
    1.000000, 1.627558, 1.274203,
    1.000000, 0.716164, 0.398462,
    1.000000, 1.051941, 1.274203,
    1.000000, 0.668196, 0.945800,
    1.000000, 1.531622, 0.945800,
    1.000000, 0.668196, 0.726865,
    1.000000, 1.051941, 1.055268,
    1.000000, 1.195845, 0.726865,
    1.000000, 0.620228, 0.726865,
    1.000000, 0.668196, 0.726865,
    1.000000, 1.003973, 1.055268,
    1.000000, 1.099909, 0.507930,
    1.000000, 1.243813, 0.836333,
    1.000000, 1.387717, 0.945800,
    1.000000, 1.003973, 1.164736,
    1.000000, 0.764132, 0.398462,
    1.000000, 1.003973, 0.288995,
    1.000000, 1.243813, 1.274203,
    1.000000, 1.003973, 1.383671,
    1.000000, 0.956004, 0.726865,
    1.000000, 0.620228, 0.726865,
    1.000000, 0.908036, 1.055268,
    1.000000, 1.003973, 1.383671,
    1.000000, 0.764132, 1.274203,
    1.000000, 0.764132, 0.836333,
    1.000000, 1.147877, 1.274203,
    1.000000, 1.051941, 1.493139,
    1.000000, 0.812100, 1.274203,
    1.000000, 0.716164, 0.836333,
    1.000000, 0.812100, 0.945800,
    1.000000, 0.908036, 1.274203,
    1.000000, 0.764132, 0.726865
];

var y: int[] = [
    0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1
];


var weights = [0.0, 0.0, 0.0]; 
var learning_rate = 0.05;
var iterations = 100;
var m = length(X) / length(weights);
var n = length(weights);

X = normalize(X, m, n);

gradient_decent(X, y, weights, m, n, learning_rate, iterations);

for var i = 0; i < m; i += 1 {
  var x: float[] = [];
  for var j = 0; j < n; j += 1 {
      push(x, X[i * n + j]);
  }
  var p: float = predict(x, weights, n);
}

var acc = evaluate(X, y, weights, m, n);
var percent = 100.0 * acc;

print "w0 = ", weights[0];
print "w1 = ", weights[1];
print "w2 = ", weights[2];

print "final accuracy: ", percent, "%";