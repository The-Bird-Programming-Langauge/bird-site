fn exp(x: float) -> float {
  if x < -30.0 {
      return 0.0;
  } else if x > 30.0 {
      return exp(30.0);
  }

  var result = 1.0;
  var term = 1.0;
  for var i = 1; i < 30; i += 1 {
      term = term * x / i as float;
      result += term;
  }
  return result;
}

fn sqrt(x: float) -> float {
  var z: float = 1.0;
  for var i = 1; i <= 30; i += 1 {
      z -= (z*z - x) / (2.0 *z); // MAGIC LINE!!
  }
  return z;
}

fn log(x: float) -> float {
  var y = (x - 1.0) / (x + 1.0);
  var y_squared = y * y;
  var result = 0.0;

  for var i = 1; i < 30; i += 2 {
      var x = 1.0 / i as float;
      for var j = 1; j < i; j += 1 {
          x *= y_squared;
      }
      result += x;
  }

  return 2.0 * y * result;
}

fn sigmoid(x: float) -> float {
  return 1.0 / (1.0 + exp(-x));
}

fn abs(x: float) ->  float {
return (x < 0.0) ? (-x) : x;
}

fn predict(x: float[], weights: float[], n: int) -> float {
  var z = 0.0;
  for var i = 0; i < n; i += 1 {
      z += weights[i] * x[i];
  }
  return sigmoid(z);
}

fn evaluate(X: float[], y: int[], weights: float[], m: int, n: int) -> float {
  var correct = 0;
  var acc = 0.0;
  for var i = 0; i < m; i += 1 {
      var x: float[] = [];
      for var j = 0; j < n; j += 1 {
          push(x, (X[i * n + j]));
      }

      var p = predict(x, weights, n);
      var predicted = 0;
      if p >= 0.5 {
          predicted = 1;
      }

      if predicted == y[i] {
          correct += 1;
      }
  }

  acc = correct as float / m as float;
  return acc;
}

fn gradient_decent(X: float[], y: int[], weights: float[], m: int, 
                 n: int, learning_rate: float, iterations: int) -> void {
  var prev = 0.0;
  for var i = 0; i < iterations; i += 1 {
      for var j = 0; j < n; j += 1 {
          var gradient = 0.0;

          for var i = 0; i < m; i += 1 {
              var z: float = 0.0;
              for var k = 0; k < n; k += 1 {
                  z += weights[k] * X[i * n + k];
              }

              var h = sigmoid(z);
              gradient += (h - y[i] as float) * X[i * n + j];
          }

          weights[j] -= learning_rate * gradient / m as float;
      }
      if i % 50 == 0 {
          var acc = evaluate(X, y, weights, m, n);
          var percent = 100.0 * acc;
          if (acc != prev) {
            print "accuracy: ", percent, "%";
          }
          prev = acc;
          if acc == 1.0 {
              print "converged!";
              return;
          }
      }
  }
}

fn normalize(X: float[], m: int, n: int) -> float[] {
  var means: float[] = [];
  var stds: float[] = [];

  push(means, 0.0);
  push(stds, 1.0);

  for var j = 1; j < n; j += 1 {
      var sum = 0.0;
      for var i = 0; i < m; i += 1 {
          sum += X[i * n + j];
      }
      var mean = sum / m as float;
      push(means, mean);

      var sq_sum = 0.0;
      for var i = 0; i < m; i += 1 {
          var diff = X[i * n + j] - mean;
          sq_sum += diff * diff;
      }
      var std = sqrt(sq_sum / m as float);
      if std == 0.0 {
          std = 1.0;
      }
      push(stds, std);
  }

  var X_norm: float[] = [];
  for var i = 0; i < m; i += 1 {
      for var j = 0; j < n; j += 1 {
          var val = X[i * n + j];
          var normalized = (j == 0) ? 1.0 : (val - means[j]) / stds[j];
          push(X_norm, normalized);
      }
  }

  return X_norm;
}

var X: float[] = [
    1.000000, -0.250779, -0.653039,
    1.000000, -0.494254, -0.416431,
    1.000000, -0.007304, -0.416431,
    1.000000, -1.102941, -0.889647,
    1.000000, -0.372516, -0.416431,
    1.000000, -0.494254, -0.889647,
    1.000000, -0.250779, -0.179822,
    1.000000, -1.955103, -1.599473,
    1.000000, -0.372516, -0.889647,
    1.000000, -1.224678, -0.653039,
    1.000000, -1.711628, -1.599473,
    1.000000, -0.859466, -0.416431,
    1.000000, -1.102941, -1.599473,
    1.000000, -0.250779, -0.653039,
    1.000000, -1.589891, -0.889647,
    1.000000, -0.615991, -0.653039,
    1.000000, -0.494254, -0.416431,
    1.000000, -0.981204, -1.599473,
    1.000000, -0.494254, -0.416431,
    1.000000, -1.224678, -1.362864,
    1.000000, -0.129042, 0.293394,
    1.000000, -1.102941, -0.889647,
    1.000000, -0.007304, -0.416431,
    1.000000, -0.250779, -1.126256,
    1.000000, -0.737729, -0.889647,
    1.000000, -0.615991, -0.653039,
    1.000000, -0.129042, -0.653039,
    1.000000, 0.114433, 0.056786,
    1.000000, -0.494254, -0.416431,
    1.000000, -1.711628, -1.599473,
    1.000000, -1.346416, -1.362864,
    1.000000, -1.468153, -1.599473,
    1.000000, -1.224678, -1.126256,
    1.000000, 0.236171, -0.179822,
    1.000000, -0.494254, -0.416431,
    1.000000, -0.494254, -0.179822,
    1.000000, -0.250779, -0.416431,
    1.000000, -0.615991, -0.889647,
    1.000000, -0.981204, -0.889647,
    1.000000, -1.102941, -0.889647,
    1.000000, -0.615991, -1.126256,
    1.000000, -0.372516, -0.653039,
    1.000000, -1.102941, -1.126256,
    1.000000, -1.955103, -1.599473,
    1.000000, -0.859466, -0.889647,
    1.000000, -0.859466, -1.126256,
    1.000000, -0.859466, -0.889647,
    1.000000, -0.737729, -0.889647,
    1.000000, -2.320315, -1.362864,
    1.000000, -0.981204, -0.889647,
    1.000000, 1.331807, 1.949653,
    1.000000, 0.236171, 0.530003,
    1.000000, 1.210070, 1.003219,
    1.000000, 0.844858, 0.293394,
    1.000000, 1.088332, 1.239828,
    1.000000, 2.062232, 1.003219,
    1.000000, -0.494254, 0.056786,
    1.000000, 1.697019, 0.293394,
    1.000000, 1.088332, 0.293394,
    1.000000, 1.453545, 1.949653,
    1.000000, 0.236171, 0.766611,
    1.000000, 0.479645, 0.530003,
    1.000000, 0.723120, 1.003219,
    1.000000, 0.114433, 0.766611,
    1.000000, 0.236171, 1.713045,
    1.000000, 0.479645, 1.476436,
    1.000000, 0.723120, 0.293394,
    1.000000, 2.183969, 1.239828,
    1.000000, 2.427444, 1.476436,
    1.000000, 0.114433, -0.416431,
    1.000000, 0.966595, 1.476436,
    1.000000, -0.007304, 0.766611,
    1.000000, 2.183969, 0.766611,
    1.000000, -0.007304, 0.293394,
    1.000000, 0.966595, 1.003219,
    1.000000, 1.331807, 0.293394,
    1.000000, -0.129042, 0.293394,
    1.000000, -0.007304, 0.293394,
    1.000000, 0.844858, 1.003219,
    1.000000, 1.088332, -0.179822,
    1.000000, 1.453545, 0.530003,
    1.000000, 1.818757, 0.766611,
    1.000000, 0.844858, 1.239828,
    1.000000, 0.236171, -0.416431,
    1.000000, 0.844858, -0.653039,
    1.000000, 1.453545, 1.476436,
    1.000000, 0.844858, 1.713045,
    1.000000, 0.723120, 0.293394,
    1.000000, -0.129042, 0.293394,
    1.000000, 0.601383, 1.003219,
    1.000000, 0.844858, 1.713045,
    1.000000, 0.236171, 1.476436,
    1.000000, 0.236171, 0.530003,
    1.000000, 1.210070, 1.476436,
    1.000000, 0.966595, 1.949653,
    1.000000, 0.357908, 1.476436,
    1.000000, 0.114433, 0.530003,
    1.000000, 0.357908, 0.766611,
    1.000000, 0.601383, 1.476436,
    1.000000, 0.236171, 0.293394
];

var y: int[] = [
    0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1
];

var weights = [0.0, 0.0, 0.0]; 
var learning_rate = 0.12;
var iterations = 100;
var m = length(X) / length(weights);
var n = length(weights);

X = normalize(X, m, n);

gradient_decent(X, y, weights, m, n, learning_rate, iterations);

for var i = 0; i < m; i += 1 {
  var x: float[] = [];
  for var j = 0; j < n; j += 1 {
      push(x, X[i * n + j]);
  }
  var p: float = predict(x, weights, n);
}

var acc = evaluate(X, y, weights, m, n);
var percent = 100.0 * acc;

print "w0 = ", weights[0];
print "w1 = ", weights[1];
print "w2 = ", weights[2];

print "final accuracy: ", percent, "%";